{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5G Network Anomaly Detection Demo\n",
    "\n",
    "This notebook demonstrates the complete pipeline for detecting anomalies in 5G network metrics using autoencoders and generating natural language reports with LLMs.\n",
    "\n",
    "## Overview\n",
    "1. **Data Generation**: Create synthetic 5G network metrics\n",
    "2. **Model Training**: Train an autoencoder for anomaly detection\n",
    "3. **Anomaly Detection**: Detect anomalies in new data\n",
    "4. **LLM Reporting**: Generate natural language reports\n",
    "5. **Visualization**: Create comprehensive plots and analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('.')\n",
    "\n",
    "# Import our custom modules\n",
    "from data.generate_synthetic_data import generate_synthetic_5g_data\n",
    "from models.anomaly_detector import AnomalyDetector, plot_training_history, plot_reconstruction_errors\n",
    "from models.llm_reporter import NetworkAnomalyReporter\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"✅ All imports successful!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation and Exploration\n",
    "\n",
    "Let's generate synthetic 5G network metrics with known anomalies for demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic 5G network data\n",
    "print(\"📊 Generating synthetic 5G network metrics...\")\n",
    "\n",
    "# Create training data (less anomalies for better training)\n",
    "train_data = generate_synthetic_5g_data(num_samples=5000, anomaly_rate=0.05)\n",
    "\n",
    "# Create test data (more anomalies for better demonstration)\n",
    "test_data = generate_synthetic_5g_data(num_samples=2000, anomaly_rate=0.15)\n",
    "\n",
    "print(f\"Training data: {len(train_data)} samples, {train_data['is_anomaly'].sum()} anomalies ({train_data['is_anomaly'].mean():.1%})\")\n",
    "print(f\"Test data: {len(test_data)} samples, {test_data['is_anomaly'].sum()} anomalies ({test_data['is_anomaly'].mean():.1%})\")\n",
    "\n",
    "# Define feature columns\n",
    "feature_columns = [\n",
    "    'prb_utilization',\n",
    "    'active_ue_count', \n",
    "    'throughput_mbps',\n",
    "    'latency_ms',\n",
    "    'handover_success_rate',\n",
    "    'snr_db',\n",
    "    'packet_loss_rate'\n",
    "]\n",
    "\n",
    "print(f\"Feature columns: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the training data\n",
    "print(\"📈 Training Data Summary:\")\n",
    "display(train_data[feature_columns + ['is_anomaly']].describe())\n",
    "\n",
    "# Show sample of the data\n",
    "print(\"\\n📋 Sample Data:\")\n",
    "display(train_data[feature_columns + ['is_anomaly']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the training data distribution\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(feature_columns):\n",
    "    # Plot normal vs anomaly distributions\n",
    "    normal_data = train_data[train_data['is_anomaly'] == False][col]\n",
    "    anomaly_data = train_data[train_data['is_anomaly'] == True][col]\n",
    "    \n",
    "    axes[i].hist(normal_data, bins=50, alpha=0.7, label='Normal', density=True, color='blue')\n",
    "    axes[i].hist(anomaly_data, bins=50, alpha=0.7, label='Anomaly', density=True, color='red')\n",
    "    axes[i].set_title(f'{col.replace(\"_\", \" \").title()}')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "# Hide the last two empty subplots\n",
    "axes[7].axis('off')\n",
    "axes[8].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Distribution of 5G Network Metrics: Normal vs Anomalous', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "print(\"🎯 The red histograms show anomalous values that deviate significantly from normal patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training\n",
    "\n",
    "Now let's train our autoencoder model on normal network data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the anomaly detector\n",
    "print(\"🤖 Initializing anomaly detector...\")\n",
    "\n",
    "detector = AnomalyDetector(\n",
    "    input_dim=len(feature_columns),\n",
    "    encoding_dim=16,  # Smaller for demo\n",
    "    hidden_dim=32     # Smaller for demo\n",
    ")\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(f\"- Input dimension: {len(feature_columns)}\")\n",
    "print(f\"- Hidden dimension: 32\")\n",
    "print(f\"- Encoding dimension: 16\")\n",
    "print(f\"- Device: {detector.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "print(\"📚 Preparing training data...\")\n",
    "\n",
    "train_loader, val_loader = detector.prepare_data(\n",
    "    train_data, \n",
    "    feature_columns, \n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Validation samples: {len(val_loader.dataset)}\")\n",
    "print(\"✅ Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"🚀 Starting model training...\")\n",
    "\n",
    "history = detector.train(\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    epochs=50,  # Fewer epochs for demo\n",
    "    learning_rate=0.001,\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Training completed!\")\n",
    "print(f\"Epochs trained: {history['epochs_trained']}\")\n",
    "print(f\"Final training loss: {history['train_losses'][-1]:.6f}\")\n",
    "print(f\"Final validation loss: {history['val_losses'][-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plot_training_history(history)\n",
    "plt.title('Autoencoder Training Progress')\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 The training loss should decrease over time, indicating the model is learning the normal patterns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Anomaly Detection\n",
    "\n",
    "Let's calculate the anomaly threshold and test our model on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate anomaly threshold\n",
    "print(\"🎯 Calculating anomaly threshold...\")\n",
    "\n",
    "threshold = detector.calculate_threshold(train_data, feature_columns, percentile=95)\n",
    "print(f\"Anomaly threshold set to: {threshold:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model on test data\n",
    "print(\"🔍 Evaluating model on test data...\")\n",
    "\n",
    "metrics = detector.evaluate_model(test_data, feature_columns)\n",
    "\n",
    "print(\"\\n📊 Model Performance Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"- {metric.capitalize()}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get reconstruction errors for visualization\n",
    "reconstruction_errors = detector.detect_anomalies(test_data, feature_columns)\n",
    "true_labels = test_data['is_anomaly'].values\n",
    "\n",
    "# Plot reconstruction errors\n",
    "plot_reconstruction_errors(reconstruction_errors, true_labels, threshold)\n",
    "plt.suptitle('Reconstruction Error Analysis', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "print(\"🎯 Left plot shows the distribution - anomalies should have higher reconstruction errors.\")\n",
    "print(\"📈 Right plot shows errors over time - red dots are true anomalies.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed anomaly analysis\n",
    "predictions = reconstruction_errors > threshold\n",
    "anomaly_indices = np.where(predictions)[0]\n",
    "\n",
    "print(f\"🔍 Anomaly Detection Results:\")\n",
    "print(f\"- Total samples analyzed: {len(test_data)}\")\n",
    "print(f\"- True anomalies: {true_labels.sum()}\")\n",
    "print(f\"- Predicted anomalies: {predictions.sum()}\")\n",
    "print(f\"- Accuracy: {((predictions == true_labels).sum() / len(true_labels)):.3f}\")\n",
    "\n",
    "# Show some detected anomalies\n",
    "if len(anomaly_indices) > 0:\n",
    "    print(f\"\\n📋 Sample of detected anomalies:\")\n",
    "    anomaly_samples = test_data.iloc[anomaly_indices[:5]][feature_columns + ['is_anomaly']]\n",
    "    anomaly_samples['reconstruction_error'] = reconstruction_errors[anomaly_indices[:5]]\n",
    "    display(anomaly_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LLM Report Generation\n",
    "\n",
    "Now let's generate natural language reports about the detected anomalies using our LLM reporter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LLM reporter\n",
    "print(\"🤖 Initializing LLM reporter...\")\n",
    "\n",
    "reporter = NetworkAnomalyReporter(model_name=\"google/flan-t5-small\")\n",
    "\n",
    "print(\"✅ LLM reporter initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive anomaly report\n",
    "print(\"📝 Generating comprehensive anomaly report...\")\n",
    "\n",
    "report = reporter.create_detailed_report(\n",
    "    test_data, \n",
    "    anomaly_indices,\n",
    "    timestamp=datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    ")\n",
    "\n",
    "print(\"✅ Report generation complete!\")\n",
    "print(f\"Report covers {len(anomaly_indices)} detected anomalies out of {len(test_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated report\n",
    "print(\"=\"*80)\n",
    "print(\"📊 5G NETWORK ANOMALY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\n🕐 Timestamp: {report['timestamp']}\")\n",
    "print(f\"📈 Statistics: {report['anomaly_statistics']['total_anomalies']} anomalies out of {report['anomaly_statistics']['total_samples']} samples ({report['anomaly_statistics']['anomaly_rate']})\")\n",
    "\n",
    "print(f\"\\n📋 EXECUTIVE SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "print(report['executive_summary'])\n",
    "\n",
    "print(f\"\\n🔧 TECHNICAL DETAILS:\")\n",
    "print(\"-\" * 40)\n",
    "print(report['technical_details'])\n",
    "\n",
    "print(f\"\\n💡 RECOMMENDATIONS:\")\n",
    "print(\"-\" * 40)\n",
    "print(report['recommendations'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate real-time alerts for critical issues\n",
    "print(\"🚨 Generating real-time alerts for critical issues...\")\n",
    "\n",
    "alerts = reporter.generate_real_time_alerts(test_data, anomaly_indices)\n",
    "\n",
    "if alerts:\n",
    "    print(f\"\\n⚠️  CRITICAL ALERTS ({len(alerts)} alerts):\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, alert in enumerate(alerts, 1):\n",
    "        print(f\"\\n🚨 Alert #{i} - {alert['metric'].upper()}:\")\n",
    "        print(f\"   Severity: {alert['severity'].upper()}\")\n",
    "        print(f\"   Affected samples: {alert['affected_samples']}\")\n",
    "        print(f\"   Message: {alert['message']}\")\n",
    "        print(f\"   Timestamp: {alert['timestamp']}\")\nelse:\n",
    "    print(\"✅ No critical alerts generated - all metrics within acceptable ranges.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Visualizations\n",
    "\n",
    "Let's create comprehensive visualizations to better understand our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a correlation matrix of features\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = test_data[feature_columns].corr()\n",
    "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', center=0,\n",
    "            square=True, fmt='.2f')\n",
    "plt.title('Correlation Matrix of 5G Network Metrics')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"🔍 This shows how different network metrics correlate with each other.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection performance visualization\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Confusion Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: Confusion Matrix\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Confusion Matrix')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xticklabels(['Normal', 'Anomaly'])\n",
    "axes[0].set_yticklabels(['Normal', 'Anomaly'])\n",
    "\n",
    "# Plot 2: ROC Curve\n",
    "fpr, tpr, _ = roc_curve(true_labels, reconstruction_errors)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "axes[1].plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "axes[1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.8)\n",
    "axes[1].set_xlim([0.0, 1.0])\n",
    "axes[1].set_ylim([0.0, 1.05])\n",
    "axes[1].set_xlabel('False Positive Rate')\n",
    "axes[1].set_ylabel('True Positive Rate')\n",
    "axes[1].set_title('ROC Curve')\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"🎯 Confusion Matrix shows classification results.\")\n",
    "print(f\"📈 ROC AUC of {roc_auc:.3f} indicates {'excellent' if roc_auc > 0.9 else 'good' if roc_auc > 0.8 else 'fair'} performance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series visualization of anomalies\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 12))\n",
    "\n",
    "# Convert timestamps to datetime for plotting\n",
    "test_data['datetime'] = pd.to_datetime(test_data['timestamp'])\n",
    "time_index = range(len(test_data))\n",
    "\n",
    "# Plot 1: Key metrics over time with anomalies highlighted\n",
    "axes[0].plot(time_index, test_data['throughput_mbps'], alpha=0.7, label='Throughput (Mbps)')\n",
    "axes[0].plot(time_index, test_data['latency_ms'] * 10, alpha=0.7, label='Latency (ms × 10)')\n",
    "anomaly_mask = test_data['is_anomaly'] == True\n",
    "axes[0].scatter(np.array(time_index)[anomaly_mask], test_data['throughput_mbps'][anomaly_mask], \n",
    "                color='red', alpha=0.8, s=50, label='True Anomalies', zorder=5)\n",
    "axes[0].set_title('Network Performance Over Time')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Reconstruction errors with threshold\n",
    "axes[1].plot(time_index, reconstruction_errors, alpha=0.8, color='blue', label='Reconstruction Error')\n",
    "axes[1].axhline(y=threshold, color='red', linestyle='--', alpha=0.8, \n",
    "                label=f'Threshold ({threshold:.4f})')\n",
    "axes[1].fill_between(time_index, 0, threshold, alpha=0.2, color='green', label='Normal Range')\n",
    "axes[1].fill_between(time_index, threshold, reconstruction_errors.max(), \n",
    "                     alpha=0.2, color='red', label='Anomaly Range')\n",
    "axes[1].set_title('Reconstruction Errors and Anomaly Threshold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Multiple metrics comparison\n",
    "# Normalize metrics for comparison\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(test_data[feature_columns[:4]])\n",
    "\n",
    "for i, col in enumerate(feature_columns[:4]):\n",
    "    axes[2].plot(time_index, normalized_data[:, i], alpha=0.7, label=col)\n",
    "\n",
    "axes[2].scatter(np.array(time_index)[anomaly_mask], \n",
    "                np.ones(anomaly_mask.sum()) * 1.1, \n",
    "                color='red', alpha=0.8, s=30, marker='v', label='Anomalies')\n",
    "axes[2].set_title('Normalized Metrics Comparison')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_ylim(-0.1, 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 These plots show how anomalies manifest in different metrics over time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "# Calculate which features contribute most to anomaly detection\n",
    "feature_anomaly_correlation = []\n",
    "\n",
    "for col in feature_columns:\n",
    "    # Calculate correlation between feature values and being anomalous\n",
    "    correlation = np.corrcoef(test_data[col], reconstruction_errors)[0, 1]\n",
    "    feature_anomaly_correlation.append(abs(correlation))\n",
    "\n",
    "# Create feature importance plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': [col.replace('_', ' ').title() for col in feature_columns],\n",
    "    'Importance': feature_anomaly_correlation\n",
    "}).sort_values('Importance', ascending=True)\n",
    "\n",
    "bars = plt.barh(feature_importance_df['Feature'], feature_importance_df['Importance'], \n",
    "                color=plt.cm.viridis(np.linspace(0, 1, len(feature_columns))))\n",
    "plt.xlabel('Correlation with Reconstruction Error (Absolute Value)')\n",
    "plt.title('Feature Importance for Anomaly Detection')\n",
    "plt.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (bar, value) in enumerate(zip(bars, feature_importance_df['Importance'])):\n",
    "    plt.text(value + 0.001, bar.get_y() + bar.get_height()/2, \n",
    "             f'{value:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 Features with higher correlation contribute more to anomaly detection.\")\n",
    "print(f\"Most important feature: {feature_importance_df.iloc[-1]['Feature']}\")\n",
    "print(f\"Least important feature: {feature_importance_df.iloc[0]['Feature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-world Application Simulation\n",
    "\n",
    "Let's simulate how this system would work in a real-world monitoring scenario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate real-time monitoring\n",
    "print(\"🔄 Simulating real-time network monitoring...\")\n",
    "\n",
    "# Generate a small batch of \"live\" data\n",
    "live_data = generate_synthetic_5g_data(num_samples=50, anomaly_rate=0.2)\n",
    "\n",
    "# Process each sample as if it's arriving in real-time\n",
    "live_results = []\n",
    "\n",
    "for i, (idx, sample) in enumerate(live_data.iterrows()):\n",
    "    # Create a single-sample DataFrame\n",
    "    sample_df = pd.DataFrame([sample[feature_columns]], columns=feature_columns)\n",
    "    \n",
    "    # Detect anomaly\n",
    "    error, prediction = detector.predict_anomalies(sample_df, feature_columns)\n",
    "    \n",
    "    result = {\n",
    "        'timestamp': sample['timestamp'],\n",
    "        'is_anomaly': bool(prediction[0]),\n",
    "        'true_anomaly': sample['is_anomaly'],\n",
    "        'reconstruction_error': float(error[0]),\n",
    "        'confidence': abs(float(error[0]) - threshold) / threshold\n",
    "    }\n",
    "    \n",
    "    # Add key metrics\n",
    "    for col in feature_columns[:3]:  # Just show first 3 for brevity\n",
    "        result[col] = sample[col]\n",
    "    \n",
    "    live_results.append(result)\n",
    "    \n",
    "    # Show progress\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Processed {i + 1}/50 samples...\")\n",
    "\n",
    "live_results_df = pd.DataFrame(live_results)\n",
    "print(\"\\n✅ Real-time simulation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display real-time monitoring results\n",
    "detected_anomalies = live_results_df[live_results_df['is_anomaly'] == True]\n",
    "\n",
    "print(f\"🎯 Real-time Monitoring Summary:\")\n",
    "print(f\"- Total samples processed: {len(live_results_df)}\")\n",
    "print(f\"- Anomalies detected: {len(detected_anomalies)}\")\n",
    "print(f\"- True anomalies: {live_results_df['true_anomaly'].sum()}\")\n",
    "print(f\"- Accuracy: {((live_results_df['is_anomaly'] == live_results_df['true_anomaly']).sum() / len(live_results_df)):.2%}\")\n",
    "\n",
    "if len(detected_anomalies) > 0:\n",
    "    print(f\"\\n🚨 Detected Anomalies:\")\n",
    "    display(detected_anomalies[['timestamp', 'reconstruction_error', 'confidence', \n",
    "                               'prb_utilization', 'throughput_mbps', 'latency_ms']].head())\nelse:\n",
    "    print(\"✅ No anomalies detected in this batch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a quick report for the live monitoring session\n",
    "if len(detected_anomalies) > 0:\n",
    "    print(\"📝 Generating quick report for live monitoring session...\")\n",
    "    \n",
    "    # Get anomaly indices for the live data\n",
    "    live_anomaly_indices = np.where(live_results_df['is_anomaly'].values)[0]\n",
    "    \n",
    "    # Generate alerts for critical issues\n",
    "    live_alerts = reporter.generate_real_time_alerts(\n",
    "        live_data.iloc[live_anomaly_indices], \n",
    "        range(len(live_anomaly_indices))\n",
    "    )\n",
    "    \n",
    "    if live_alerts:\n",
    "        print(\"\\n🚨 LIVE MONITORING ALERTS:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for alert in live_alerts:\n",
    "            print(f\"\\n⚠️  {alert['metric'].upper()}: {alert['message'][:80]}...\")\n",
    "    else:\n",
    "        print(\"\\n✅ No critical alerts for this monitoring session.\")\nelse:\n",
    "    print(\"✅ No anomalies detected - system operating normally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps\n",
    "\n",
    "This demo has shown the complete pipeline for 5G network anomaly detection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"🎉 DEMO COMPLETION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n✅ Successfully demonstrated:\")\n",
    "print(f\"   📊 Synthetic 5G data generation with {len(feature_columns)} metrics\")\n",
    "print(f\"   🤖 Autoencoder training with {history['epochs_trained']} epochs\")\n",
    "print(f\"   🎯 Anomaly detection with {metrics['f1_score']:.3f} F1-score\")\n",
    "print(f\"   📝 LLM report generation with natural language insights\")\n",
    "print(f\"   📈 Comprehensive visualizations and analysis\")\n",
    "print(f\"   🔄 Real-time monitoring simulation\")\n",
    "\n",
    "print(f\"\\n📈 Model Performance:\")\n",
    "print(f\"   - F1 Score: {metrics['f1_score']:.4f}\")\n",
    "print(f\"   - Precision: {metrics['precision']:.4f}\")\n",
    "print(f\"   - Recall: {metrics['recall']:.4f}\")\n",
    "print(f\"   - AUC: {metrics['auc_score']:.4f}\")\n",
    "\n",
    "print(f\"\\n🚀 Next Steps for Production:\")\n",
    "print(f\"   1. Train on real network data from your infrastructure\")\n",
    "print(f\"   2. Deploy using the FastAPI service (src/app.py)\")\n",
    "print(f\"   3. Use Kubernetes manifests for scalable deployment\")\n",
    "print(f\"   4. Integrate with monitoring systems (Prometheus, Grafana)\")\n",
    "print(f\"   5. Set up automated alerting workflows\")\n",
    "print(f\"   6. Fine-tune thresholds based on operational requirements\")\n",
    "\n",
    "print(f\"\\n💡 Key Insights:\")\n",
    "print(f\"   - Autoencoders effectively detect network anomalies\")\n",
    "print(f\"   - LLM reports provide actionable insights\")\n",
    "print(f\"   - System can handle real-time monitoring scenarios\")\n",
    "print(f\"   - Visualization helps understand model behavior\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"🎯 Demo completed successfully! Ready for production deployment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **Training Script**: Use `src/train.py` for full model training\n",
    "- **Inference Script**: Use `src/inference.py` for batch processing\n",
    "- **API Service**: Run `src/app.py` for REST API deployment\n",
    "- **Kubernetes**: Deploy using manifests in `kubernetes/` folder\n",
    "- **Docker**: Build container with the provided `Dockerfile`\n",
    "\n",
    "For questions or issues, refer to the project documentation or create an issue in the repository."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}